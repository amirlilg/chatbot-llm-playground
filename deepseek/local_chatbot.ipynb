{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyNS107llJ5LRxihJ8xq0S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirlilg/chatbot-llm-playground/blob/master/deepseek/local_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "gDoT324NNBbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install Ollama via shell commands"
      ],
      "metadata": {
        "id": "SNQC0iIaOph2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpi88bVvM_X-",
        "outputId": "52343a77-06c5-4a59-9fb0-c4539db6706f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Waiting for Ollama service to start...\n",
            "Ollama should now be running. Proceed to Cell 2.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama serve > /dev/null 2>&1 &\n",
        "# Wait for Ollama service to start\n",
        "import time\n",
        "print(\"Waiting for Ollama service to start...\")\n",
        "time.sleep(5)\n",
        "print(\"Ollama should now be running. Proceed to Cell 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Verify Ollama is running and install Python client"
      ],
      "metadata": {
        "id": "b4n5ivhbPBAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import sys\n",
        "\n",
        "def is_ollama_running():\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/tags\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"✅ Ollama is running successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ Ollama service responded with status code: {response.status_code}\")\n",
        "            return False\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"❌ Could not connect to Ollama service. Make sure it's running.\")\n",
        "        return False\n",
        "\n",
        "# Check if Ollama is running\n",
        "if is_ollama_running():\n",
        "    # Install Ollama Python client\n",
        "    !pip install ollama\n",
        "    print(\"\\nOllama Python client installed. Proceed to Cell 3.\")\n",
        "else:\n",
        "    print(\"\\nPlease make sure Ollama is running before proceeding. Try rerunning Cell 1.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exmnmaowNKPF",
        "outputId": "986d3a85-2943-4c4a-ac63-8833214a8a64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Ollama is running successfully!\n",
            "Collecting ollama\n",
            "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.11.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
            "Downloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama\n",
            "Successfully installed ollama-0.4.8\n",
            "\n",
            "Ollama Python client installed. Proceed to Cell 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Pull DeepSeek-R1 model"
      ],
      "metadata": {
        "id": "knJFKkWCPOrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "desired_model = \"deepseek-r1:1.5b\"\n",
        "\n",
        "try:\n",
        "    print(\"Pulling DeepSeek-R1 model...\")\n",
        "    ollama.pull(desired_model)\n",
        "    print(\"✅ DeepSeek-R1 model downloaded successfully!\")\n",
        "    print(\"\\nYou can now proceed to Cell 4 to verify and use the model.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error downloading model: {e}\")\n",
        "    print(\"Please check your connection and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgRR2Zx_PB26",
        "outputId": "6a33f968-44ba-4961-e1c5-9c9194251220"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulling DeepSeek-R1 model...\n",
            "✅ DeepSeek-R1 model downloaded successfully!\n",
            "\n",
            "You can now proceed to Cell 4 to verify and use the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Verify available models"
      ],
      "metadata": {
        "id": "CdctN87pPd3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import json\n",
        "\n",
        "try:\n",
        "    models = ollama.list()\n",
        "    print(f\"{models=}\")\n",
        "    print(\"Available models:\")\n",
        "    for model in models['models']:\n",
        "        print(f\"- {model}\")\n",
        "\n",
        "    # Check if DeepSeek model is available\n",
        "#     if any(model['name'] == desired_model for model in models['models']):\n",
        "#         print(\"\\nDeepSeek-R1 model is available and ready to use!\")\n",
        "#         print(\"You can now proceed to Cell 5 to chat with the model.\")\n",
        "#     else:\n",
        "#         print(\"\\nDeepSeek-R1 model not found. Please run Cell 3 again.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error listing models: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIj2709_POE3",
        "outputId": "e4025ed9-5357-4fa5-99ff-39d17f69f05b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models=ListResponse(models=[Model(model='deepseek-r1:1.5b', modified_at=datetime.datetime(2025, 5, 21, 14, 40, 1, 26146, tzinfo=TzInfo(UTC)), digest='a42b25d8c10a841bd24724309898ae851466696a7d7f3a0a408b895538ccbc96', size=1117322599, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='1.8B', quantization_level='Q4_K_M'))])\n",
            "Available models:\n",
            "- model='deepseek-r1:1.5b' modified_at=datetime.datetime(2025, 5, 21, 14, 40, 1, 26146, tzinfo=TzInfo(UTC)) digest='a42b25d8c10a841bd24724309898ae851466696a7d7f3a0a408b895538ccbc96' size=1117322599 details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='1.8B', quantization_level='Q4_K_M')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Simple command-line chat interface"
      ],
      "metadata": {
        "id": "Pgw1gEwFQJP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "def chat_with_deepseek():\n",
        "    print(\"Starting chat with DeepSeek-R1. Type 'exit' to end the conversation.\\n\")\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    # Optional: Set a system prompt\n",
        "    system_prompt = input(\"Enter a system prompt (or press Enter to skip): \")\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Ending conversation. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=desired_model,\n",
        "                messages=messages\n",
        "            )\n",
        "\n",
        "            assistant_message = response['message']\n",
        "            messages.append(assistant_message)\n",
        "\n",
        "            print(f\"\\nDeepSeek-R1: {assistant_message['content']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "try:\n",
        "    chat_with_deepseek()\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing chat: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7E6lulKQI6y",
        "outputId": "b0268c01-39a4-4b59-985f-d12f9d4c4727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chat with DeepSeek-R1. Type 'exit' to end the conversation.\n",
            "\n",
            "Enter a system prompt (or press Enter to skip): \n",
            "\n",
            "You: write a simple code in python.\n",
            "\n",
            "DeepSeek-R1: <think>\n",
            "Alright, the user asked for a simple Python code. I should choose something that's easy to understand and execute.\n",
            "\n",
            "I'll start with a basic example since it's straightforward. Let me consider using variables and a print statement, which are fundamental concepts.\n",
            "\n",
            "I'll write an example where two integers are assigned to variables x and y. Then, I'll add them together and store the result in z. This demonstrates variable assignment and arithmetic operations.\n",
            "\n",
            "Including a print statement will show how to display the result, making it more illustrative for someone learning programming basics.\n",
            "\n",
            "Putting it all together, I can create a short script that runs when the user inputs Enter or clicks enter. This gives immediate feedback and shows how to handle input in Python.\n",
            "\n",
            "I'll structure the code with proper indentation since it's Python, ensuring each line is correct for syntax.\n",
            "\n",
            "Finally, by explaining what the code does, I provide value beyond just showing the example. Users can see the practical application of variables and operations.\n",
            "</think>\n",
            "\n",
            "Here’s a simple Python program that adds two numbers and prints the result:\n",
            "\n",
            "```python\n",
            "x = 5\n",
            "y = 3\n",
            "z = x + y\n",
            "print(z)\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "1. **Variable Assignment**: We assign `x` the value of 5 and `y` the value of 3.\n",
            "2. **Arithmetic Operation**: We add the values of `x` and `y`, storing the result in `z`.\n",
            "3. **Print Statement**: We use `print()` to display the result.\n",
            "\n",
            "When you run this program, it will output:\n",
            "\n",
            "```\n",
            "8\n",
            "```\n",
            "\n",
            "This is a fundamental example that demonstrates basic programming concepts like variable assignment, arithmetic operations, and input/output handling.\n",
            "\n",
            "You: how can i introduce tools to you (client) via model context protocol?\n",
            "\n",
            "DeepSeek-R1: <think>\n",
            "Alright, so I'm trying to figure out how to introduce tools to my client using the Model Context Protocol, or MCPP for short. I remember that this is a method used in cloud computing frameworks like AWS Lambda, Azure Functions, and Google Cloud Functionals. But I'm not entirely sure how it works, so I need to break it down step by step.\n",
            "\n",
            "First, I think I need to understand what the Model Context Protocol is all about. From what I gather, it's a way to define models in a serverless function or lambda context. These models are like placeholders for code that can be replaced with actual functionality at runtime. This makes functions more flexible and reusable across different environments.\n",
            "\n",
            "I wonder why this is useful. Maybe because it allows developers to reuse code snippets from different services without having to rewrite everything each time. That could save a lot of time and effort, especially when working on projects that span multiple AWS regions or use several cloud services.\n",
            "\n",
            "Now, how exactly do I introduce tools using MCPP? Do I need to define models in the tool's context first? Or is there another step involved?\n",
            "\n",
            "I recall that for Lambda, you can either write your own model within a function using functions and variables, or if it's too complex, you might use something like an API Gateway. But I'm not sure how MCPP fits into this.\n",
            "\n",
            "Maybe I should check what tools support the Model Context Protocol. For example, AWS Lambda supports MCPP, as do Azure Functions and Google Cloud Functionals. So any tool that integrates with these services could potentially use MCPP to define their own models for different contexts.\n",
            "\n",
            "If my client is using a specific tool, like AWS functions, I might need to set up the environment variables or configuration so that it can run in the MCPP context correctly. I think this involves specifying which service layer you're targeting and ensuring your tools are aware of this when executing functions.\n",
            "\n",
            "But wait, how do I actually start defining models for my tools? Do I just create a new model file within my project directory with the appropriate AWS or Google Cloud credentials, or is there more to it?\n",
            "\n",
            "I also need to consider where these tools should be registered. Should they be placed in the client's codebase as part of their own module or function, and how does that integration work across different services?\n",
            "\n",
            "Another thing I'm a bit fuzzy on is how MCPP handles model execution. Once defined, can my tool execute this model at runtime? Is there any overhead involved when defining these models for each service, or is it something that's optimized during the deployment phase?\n",
            "\n",
            "Perhaps I should look into some examples or tutorials to see how others have implemented this with tools like AWS functions or Google Cloud Functions. Maybe seeing code samples would help me understand better.\n",
            "\n",
            "I also wonder about the limitations of MCPP. Are there any serverless environments where it doesn't work as expected? Or are there best practices I need to follow when defining models in these contexts?\n",
            "\n",
            "Overall, my goal is to get my client set up correctly so that their tools can utilize these models effectively and efficiently across different cloud services. But without a clear plan or understanding of how MCPP works within the tools they're using, it's challenging to proceed.\n",
            "\n",
            "Maybe I should start by outlining the steps I believe are necessary: defining the model in the tool's context first, then integrating it into their functions, testing its execution, and ensuring proper integration across all relevant services. But I'm still not entirely sure how each step translates to actual code or what specific configurations I need to set up.\n",
            "\n",
            "I think breaking down each component would help. First, define a model within the tool's context using MCPP. Then, ensure that this model is correctly integrated into their functions. After that, test if the model executes properly and then run their functions across different services.\n",
            "</think>\n",
            "\n",
            "To introduce tools using Model Context Protocol (MCPP) in your client environment, follow these organized steps:\n",
            "\n",
            "1. **Define a Model Within Your Tool**:\n",
            "   - Locate where your tool is registered, typically within its project directory.\n",
            "   - Create a new model file with the appropriate AWS or Google Cloud credentials. This file will define your model using MCPP syntax.\n",
            "\n",
            "2. **Integrate the Model into Your Functions**:\n",
            "   - Ensure your functions are written in a serverless framework that supports MCPP. Tools like AWS Lambda, Azure Functions, and Google Cloud Functionals support this.\n",
            "   - Add your defined model as part of these functions.\n",
            "\n",
            "3. **Register Your Tool with the Environment**:\n",
            "   - In your client's environment configuration, specify your tool using the Model Context Configuration (MCC) file. This ensures your tools are aware of their service layer.\n",
            "   - Verify that the client correctly executes MCPP in your target environment.\n",
            "\n",
            "4. **Execute the Model and Functions**:\n",
            "   - After defining and integrating the model, run your client's functions across various services to observe the model execution.\n",
            "   - Monitor any potential performance impact during execution, particularly if multiple models are defined.\n",
            "\n",
            "5. **Consider Integration Across Services**:\n",
            "   - Ensure your tool is registered within the correct service layer (AWS or Google Cloud) for each environment.\n",
            "   - Test the integration across all relevant services after setting up MCPP correctly in your tools' configurations.\n",
            "\n",
            "By following these steps, you can effectively introduce tools using MCPP and enhance their functionality through reusable code across different cloud environments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsXn7RuFQUTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}