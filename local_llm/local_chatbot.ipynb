{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirlilg/chatbot-llm-playground/blob/master/deepseek/local_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDoT324NNBbV"
      },
      "source": [
        "# Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNQC0iIaOph2"
      },
      "source": [
        "1. Install Ollama via shell commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama serve > /dev/null 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpi88bVvM_X-",
        "outputId": "52343a77-06c5-4a59-9fb0-c4539db6706f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for Ollama service to start...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama should now be running. Proceed to Cell 2.\n"
          ]
        }
      ],
      "source": [
        "# Wait for Ollama service to start\n",
        "import time\n",
        "print(\"Waiting for Ollama service to start...\")\n",
        "time.sleep(5)\n",
        "print(\"Ollama should now be running. Proceed to Cell 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4n5ivhbPBAF"
      },
      "source": [
        "2. Verify Ollama is running and install Python client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exmnmaowNKPF",
        "outputId": "986d3a85-2943-4c4a-ac63-8833214a8a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Ollama is running successfully!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.4.8)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /home/codespace/.local/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /home/codespace/.local/lib/python3.12/site-packages (from ollama) (2.11.5)\n",
            "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\n",
            "Ollama Python client installed. Proceed to Cell 3.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import sys\n",
        "\n",
        "def is_ollama_running():\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/tags\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"✅ Ollama is running successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ Ollama service responded with status code: {response.status_code}\")\n",
        "            return False\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"❌ Could not connect to Ollama service. Make sure it's running.\")\n",
        "        return False\n",
        "\n",
        "# Check if Ollama is running\n",
        "if is_ollama_running():\n",
        "    # Install Ollama Python client\n",
        "    !pip install ollama\n",
        "    print(\"\\nOllama Python client installed. Proceed to Cell 3.\")\n",
        "else:\n",
        "    print(\"\\nPlease make sure Ollama is running before proceeding. Try rerunning Cell 1.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knJFKkWCPOrd"
      },
      "source": [
        "3. Pull DeepSeek-R1 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgRR2Zx_PB26",
        "outputId": "6a33f968-44ba-4961-e1c5-9c9194251220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pulling DeepSeek-R1 model...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ DeepSeek-R1 model downloaded successfully!\n",
            "\n",
            "You can now proceed to Cell 4 to verify and use the model.\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "desired_model = \"deepseek-r1:1.5b\"\n",
        "\n",
        "try:\n",
        "    print(\"Pulling DeepSeek-R1 model...\")\n",
        "    ollama.pull(desired_model)\n",
        "    print(\"✅ DeepSeek-R1 model downloaded successfully!\")\n",
        "    print(\"\\nYou can now proceed to Cell 4 to verify and use the model.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error downloading model: {e}\")\n",
        "    print(\"Please check your connection and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdctN87pPd3p"
      },
      "source": [
        "5. Verify available models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIj2709_POE3",
        "outputId": "e4025ed9-5357-4fa5-99ff-39d17f69f05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models=ListResponse(models=[Model(model='deepseek-r1:1.5b', modified_at=datetime.datetime(2025, 5, 24, 6, 46, 58, 461547, tzinfo=TzInfo(UTC)), digest='a42b25d8c10a841bd24724309898ae851466696a7d7f3a0a408b895538ccbc96', size=1117322599, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='1.8B', quantization_level='Q4_K_M'))])\n",
            "Available models:\n",
            "- model='deepseek-r1:1.5b' modified_at=datetime.datetime(2025, 5, 24, 6, 46, 58, 461547, tzinfo=TzInfo(UTC)) digest='a42b25d8c10a841bd24724309898ae851466696a7d7f3a0a408b895538ccbc96' size=1117322599 details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='1.8B', quantization_level='Q4_K_M')\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "import json\n",
        "\n",
        "try:\n",
        "    models = ollama.list()\n",
        "    print(f\"{models=}\")\n",
        "    print(\"Available models:\")\n",
        "    for model in models['models']:\n",
        "        print(f\"- {model}\")\n",
        "\n",
        "    # Check if DeepSeek model is available\n",
        "#     if any(model['name'] == desired_model for model in models['models']):\n",
        "#         print(\"\\nDeepSeek-R1 model is available and ready to use!\")\n",
        "#         print(\"You can now proceed to Cell 5 to chat with the model.\")\n",
        "#     else:\n",
        "#         print(\"\\nDeepSeek-R1 model not found. Please run Cell 3 again.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error listing models: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgw1gEwFQJP7"
      },
      "source": [
        "5. Simple command-line chat interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7E6lulKQI6y",
        "outputId": "b0268c01-39a4-4b59-985f-d12f9d4c4727"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "def chat_with_deepseek():\n",
        "    print(\"Starting chat with DeepSeek-R1. Type 'exit' to end the conversation.\\n\")\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    # Optional: Set a system prompt\n",
        "    system_prompt = input(\"Enter a system prompt (or press Enter to skip): \")\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Ending conversation. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=desired_model,\n",
        "                messages=messages\n",
        "            )\n",
        "\n",
        "            assistant_message = response['message']\n",
        "            messages.append(assistant_message)\n",
        "\n",
        "            print(f\"\\nDeepSeek-R1: {assistant_message['content']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "try:\n",
        "    chat_with_deepseek()\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing chat: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MCP Server: Filesystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VsXn7RuFQUTW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mcp\n",
            "  Using cached mcp-1.9.1-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio>=4.5 in /home/codespace/.local/lib/python3.12/site-packages (from mcp) (4.9.0)\n",
            "Collecting httpx-sse>=0.4 (from mcp)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /home/codespace/.local/lib/python3.12/site-packages (from mcp) (0.28.1)\n",
            "Collecting pydantic-settings>=2.5.2 (from mcp)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.2 in /home/codespace/.local/lib/python3.12/site-packages (from mcp) (2.11.5)\n",
            "Collecting python-multipart>=0.0.9 (from mcp)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting sse-starlette>=1.6.1 (from mcp)\n",
            "  Downloading sse_starlette-2.3.5-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting starlette>=0.27 (from mcp)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting uvicorn>=0.23.1 (from mcp)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio>=4.5->mcp) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio>=4.5->mcp) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /home/codespace/.local/lib/python3.12/site-packages (from anyio>=4.5->mcp) (4.13.2)\n",
            "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->mcp) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->mcp) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->mcp) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.2->mcp) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp) (1.1.0)\n",
            "Collecting click>=7.0 (from uvicorn>=0.23.1->mcp)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading mcp-1.9.1-py3-none-any.whl (130 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading sse_starlette-2.3.5-py3-none-any.whl (10 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Installing collected packages: python-multipart, httpx-sse, click, uvicorn, starlette, sse-starlette, pydantic-settings, mcp\n",
            "Successfully installed click-8.2.1 httpx-sse-0.4.0 mcp-1.9.1 pydantic-settings-2.9.1 python-multipart-0.0.20 sse-starlette-2.3.5 starlette-0.46.2 uvicorn-0.34.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install mcp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNyNS107llJ5LRxihJ8xq0S",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
