# -*- coding: utf-8 -*-
"""lcoal_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OwFEYpiSeQDfT45eIQHIrcdQM1rFhit2

# Chatbot

1. Install Ollama via shell commands
"""

# !curl -fsSL https://ollama.com/install.sh | sh
# !ollama serve > /dev/null 2>&1 &
# !pip install ollama 
# Wait for Ollama service to start
import time
print("Waiting for Ollama service to start...")
time.sleep(5)
print("Ollama should now be running. Proceed to Cell 2.")

"""2. Verify Ollama is running and install Python client"""

import requests
import sys

def is_ollama_running():
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            print("✅ Ollama is running successfully!")
            return True
        else:
            print(f"❌ Ollama service responded with status code: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("❌ Could not connect to Ollama service. Make sure it's running.")
        return False

# Check if Ollama is running
if is_ollama_running():
    # Install Ollama Python client
    # !pip install ollama
    print("\nOllama Python client installed. Proceed to Cell 3.")
else:
    print("\nPlease make sure Ollama is running before proceeding. Try rerunning Cell 1.")

"""3. Pull DeepSeek-R1 model"""

import ollama
desired_model = "deepseek-r1:1.5b"

try:
    print("Pulling DeepSeek-R1 model...")
    ollama.pull(desired_model)
    print("✅ DeepSeek-R1 model downloaded successfully!")
    print("\nYou can now proceed to Cell 4 to verify and use the model.")
except Exception as e:
    print(f"❌ Error downloading model: {e}")
    print("Please check your connection and try again.")

"""5. Verify available models"""

import ollama
import json

try:
    models = ollama.list()
    print(f"{models=}")
    print("Available models:")
    for model in models['models']:
        print(f"- {model}")

    # Check if DeepSeek model is available
#     if any(model['name'] == desired_model for model in models['models']):
#         print("\nDeepSeek-R1 model is available and ready to use!")
#         print("You can now proceed to Cell 5 to chat with the model.")
#     else:
#         print("\nDeepSeek-R1 model not found. Please run Cell 3 again.")
except Exception as e:
    print(f"Error listing models: {e}")

"""5. Simple command-line chat interface"""

import ollama

def chat_with_deepseek():
    print("Starting chat with DeepSeek-R1. Type 'exit' to end the conversation.\n")

    messages = []

    # Optional: Set a system prompt
    system_prompt = input("Enter a system prompt (or press Enter to skip): ")
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    while True:
        user_input = input("\nYou: ")

        if user_input.lower() == 'exit':
            print("Ending conversation. Goodbye!")
            break

        messages.append({"role": "user", "content": user_input})

        try:
            response = ollama.chat(
                model=desired_model,
                messages=messages
            )

            assistant_message = response['message']
            messages.append(assistant_message)

            print(f"\nDeepSeek-R1: {assistant_message['content']}")

        except Exception as e:
            print(f"Error: {e}")

try:
    chat_with_deepseek()
except Exception as e:
    print(f"Error initializing chat: {e}")

"""# MCP"""

# !pip install mcp

from mcp.server.fastmcp import FastMCP
import json
import requests
from typing import List

mcp = FastMCP("test")