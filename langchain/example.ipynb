{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opvC4T-qRWNQ"
      },
      "outputs": [],
      "source": [
        "# Document Q&A Bot with LangChain on Google Colab\n",
        "# This notebook implements a document QA system conceptually similar to\n",
        "# Anthropic's Model Context Protocol but using open-source components\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q langchain langchain_community langchain-huggingface\n",
        "!pip install -q faiss-cpu sentence-transformers\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "from typing import List, Dict, Any\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import Document\n",
        "\n",
        "# For running a model locally on Colab\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "class DocumentQABot:\n",
        "    def __init__(self, model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
        "        \"\"\"Initialize the Document QA Bot with local model and LangChain components.\"\"\"\n",
        "        print(\"Initializing Document QA Bot...\")\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "\n",
        "        # Load model and tokenizer with optimizations for Colab\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,  # Use fp16 for efficiency\n",
        "            device_map=\"auto\",          # Automatically use available devices\n",
        "            load_in_8bit=True           # 8-bit quantization to reduce memory usage\n",
        "        )\n",
        "\n",
        "        # Setup text generation pipeline\n",
        "        self.pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=2048,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.15\n",
        "        )\n",
        "\n",
        "        # Create LangChain LLM\n",
        "        self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "\n",
        "        # Initialize embeddings model\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
        "        )\n",
        "\n",
        "        # Initialize document store\n",
        "        self.documents = {}\n",
        "        self.vectorstore = None\n",
        "        self.conversation_chain = None\n",
        "        self.chat_history = []\n",
        "\n",
        "        # Text splitter for document processing\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "        # Memory for conversation history\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True\n",
        "        )\n",
        "\n",
        "        print(\"Document QA Bot initialized and ready!\")\n",
        "\n",
        "    def add_document(self, document_content: str, document_name: str = None) -> str:\n",
        "        \"\"\"Add a document to the context.\"\"\"\n",
        "        doc_id = str(uuid.uuid4())\n",
        "        doc_name = document_name or f\"Document-{doc_id[:8]}\"\n",
        "\n",
        "        # Store document metadata\n",
        "        self.documents[doc_id] = {\n",
        "            \"id\": doc_id,\n",
        "            \"name\": doc_name,\n",
        "            \"content\": document_content\n",
        "        }\n",
        "\n",
        "        print(f\"Processing document: {doc_name}\")\n",
        "\n",
        "        # Process the document into chunks\n",
        "        doc_obj = Document(page_content=document_content, metadata={\"source\": doc_name, \"id\": doc_id})\n",
        "        doc_chunks = self.text_splitter.split_documents([doc_obj])\n",
        "\n",
        "        # Update or create the vector store\n",
        "        if self.vectorstore is None:\n",
        "            self.vectorstore = FAISS.from_documents(doc_chunks, self.embeddings)\n",
        "        else:\n",
        "            # Add document to existing vectorstore\n",
        "            self.vectorstore.add_documents(doc_chunks)\n",
        "\n",
        "        # Create or update the conversation chain with the new document context\n",
        "        self._update_conversation_chain()\n",
        "\n",
        "        print(f\"Added document '{doc_name}' (ID: {doc_id}) to the context\")\n",
        "        return doc_id\n",
        "\n",
        "    def _update_conversation_chain(self):\n",
        "        \"\"\"Update the conversation chain with the current vectorstore.\"\"\"\n",
        "        if self.vectorstore:\n",
        "            self.conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "                llm=self.llm,\n",
        "                retriever=self.vectorstore.as_retriever(\n",
        "                    search_type=\"similarity\",\n",
        "                    search_kwargs={\"k\": 5}  # Retrieve top 5 chunks\n",
        "                ),\n",
        "                memory=self.memory,\n",
        "                return_source_documents=True,\n",
        "                verbose=True\n",
        "            )\n",
        "\n",
        "    def ask(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Ask a question about the documents in context.\"\"\"\n",
        "        if not self.conversation_chain:\n",
        "            return {\n",
        "                \"answer\": \"No documents have been added to the context yet. Please add at least one document before asking questions.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        print(f\"Question: {question}\")\n",
        "        print(\"Retrieving relevant document sections...\")\n",
        "\n",
        "        # Use the conversation chain to get an answer\n",
        "        result = self.conversation_chain({\"question\": question})\n",
        "\n",
        "        # Extract source document references\n",
        "        sources = []\n",
        "        if \"source_documents\" in result:\n",
        "            for doc in result[\"source_documents\"]:\n",
        "                sources.append({\n",
        "                    \"name\": doc.metadata.get(\"source\", \"Unknown\"),\n",
        "                    \"id\": doc.metadata.get(\"id\", \"Unknown\"),\n",
        "                    \"excerpt\": doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
        "                })\n",
        "\n",
        "        print(\"Generated answer based on document context\")\n",
        "\n",
        "        return {\n",
        "            \"answer\": result[\"answer\"],\n",
        "            \"sources\": sources\n",
        "        }\n",
        "\n",
        "    def list_documents(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"List all documents in the context.\"\"\"\n",
        "        return [{\"id\": doc_id, \"name\": doc[\"name\"]} for doc_id, doc in self.documents.items()]\n",
        "\n",
        "    def remove_document(self, doc_id: str) -> bool:\n",
        "        \"\"\"Remove a document from the context.\"\"\"\n",
        "        if doc_id in self.documents:\n",
        "            doc_name = self.documents[doc_id][\"name\"]\n",
        "            del self.documents[doc_id]\n",
        "\n",
        "            print(f\"Removing document '{doc_name}' (ID: {doc_id}) from context\")\n",
        "            print(\"Rebuilding vector store without this document...\")\n",
        "\n",
        "            # For simplicity, we'll rebuild the vectorstore from scratch\n",
        "            # A more efficient implementation would directly remove vectors from FAISS\n",
        "            self.vectorstore = None\n",
        "\n",
        "            # Re-add all remaining documents\n",
        "            remaining_docs = []\n",
        "            for d_id, doc in self.documents.items():\n",
        "                doc_obj = Document(\n",
        "                    page_content=doc[\"content\"],\n",
        "                    metadata={\"source\": doc[\"name\"], \"id\": d_id}\n",
        "                )\n",
        "                remaining_docs.append(doc_obj)\n",
        "\n",
        "            if remaining_docs:\n",
        "                doc_chunks = self.text_splitter.split_documents(remaining_docs)\n",
        "                self.vectorstore = FAISS.from_documents(doc_chunks, self.embeddings)\n",
        "                self._update_conversation_chain()\n",
        "            else:\n",
        "                self.conversation_chain = None\n",
        "\n",
        "            print(f\"Document '{doc_name}' removed from context\")\n",
        "            return True\n",
        "\n",
        "        print(f\"Document with ID {doc_id} not found\")\n",
        "        return False\n",
        "\n",
        "    def clear_conversation_history(self):\n",
        "        \"\"\"Clear the conversation history.\"\"\"\n",
        "        self.memory.clear()\n",
        "        print(\"Conversation history cleared\")\n",
        "\n",
        "# Run this cell to initialize the bot\n",
        "# If you have limited resources, use a smaller model\n",
        "# bot = DocumentQABot(\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\")  # Smaller model\n",
        "bot = DocumentQABot(\"mistralai/Mistral-7B-Instruct-v0.2\")        # Standard model\n",
        "\n",
        "# Add your own documents here\n",
        "doc1_id = bot.add_document(\"\"\"\n",
        "# Company Overview\n",
        "Acme Corporation was founded in 2010 and specializes in AI solutions for healthcare.\n",
        "Our annual revenue reached $50 million in 2023, with a 25% year-over-year growth.\n",
        "\n",
        "## Products\n",
        "- MedAssist: AI diagnostic tool\n",
        "- HealthTracker: Patient monitoring system\n",
        "- DocFlow: Medical documentation automation\n",
        "\n",
        "## Leadership\n",
        "- CEO: Jane Smith\n",
        "- CTO: John Davis\n",
        "- CFO: Michael Johnson\n",
        "\"\"\", \"Acme Company Overview\")\n",
        "\n",
        "doc2_id = bot.add_document(\"\"\"\n",
        "# Q4 2023 Financial Report\n",
        "\n",
        "Acme Corporation closed Q4 with strong performance:\n",
        "- Revenue: $15.2 million (30% increase from Q3)\n",
        "- New customers: 45 hospitals and 120 clinics\n",
        "- MedAssist adoption up 40%\n",
        "\n",
        "## Challenges\n",
        "- Supply chain issues delayed HealthTracker 2.0 release\n",
        "- Increasing competition in the medical AI space\n",
        "\n",
        "## 2024 Outlook\n",
        "Planning IPO in Q3 2024 with estimated valuation of $500M\n",
        "\"\"\", \"Q4 Financial Report\")\n",
        "\n",
        "# Ask questions about your documents\n",
        "result = bot.ask(\"What products does Acme offer?\")\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(\"Sources:\")\n",
        "for source in result['sources']:\n",
        "    print(f\"- {source['name']}: {source['excerpt']}\")\n",
        "\n",
        "# Ask another question\n",
        "result = bot.ask(\"When is the company planning to go public and what's the valuation?\")\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(\"Sources:\")\n",
        "for source in result['sources']:\n",
        "    print(f\"- {source['name']}: {source['excerpt']}\")\n",
        "\n",
        "# Add your own document and ask a question about it\n",
        "# my_doc_id = bot.add_document(\"Your document text here\", \"Your Document Name\")\n",
        "# result = bot.ask(\"Your question about the document\")\n",
        "\n",
        "# Remove a document when you no longer need it\n",
        "# bot.remove_document(doc_id)\n",
        "\n",
        "# Clear conversation history if needed\n",
        "# bot.clear_conversation_history()"
      ]
    }
  ]
}